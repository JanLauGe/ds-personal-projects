---
title: "R Notebook"
output: html_notebook
---

TODO: We all knew birds nest but who knew that data frames nest, too?



```{r setup, message=FALSE}
library(here)
library(tidyverse)
library(magrittr)
library(stringr)
library(lubridate)

library(rgbif)
library(sp)
library(rgdal)
library(raster)

library(tidymodels)
library(glmnet)
library(caret)

library(rasterVis)
library(viridis)
library(ggthemes)
library(gganimate)

```


The Scottish Crossbill (**Loxia scotica**) is a small passerine bird that 
inhabits the Caledonian Forests of Scotland, and is the only terrestrial 
vertebrate species unique to the United Kingdom. Only ~ 20000 individuals of 
this species are alive today.

The first step is to get occurrence data for the species we are interested in.
This used to be the main challenge in Biogeography. Natural Historians such as
Charles Darwin, Alfred Russel Wallace, and Alexander von Humboldt would travel
for years on rustic sail ships around the globe collecting specimen. Today, 
we are standing on the shoulders of giants. Getting data is fast and easy 
thanks to the work of two organisations:

- [the Global Biodiversity Information Facility (GBIF)](https://www.gbif.org/),
an international network and research infrastructure funded by the worldâ€™s 
governments and aimed at providing anyone, anywhere, open access to data about 
all types of life on Earth.

- [rOpenSci](https://ropensci.org/), a non-profit initiative that has developed 
an ecosystem of open source tools, we run annual unconferences, and review 
community developed software.


```{r data_download}
# Cute finces with funny beaks don't only occur in the Galapagos islands
# (https://www.datacamp.com/courses/statistical-thinking-in-python-part-2)


# THIS ONLY NEEDS TO RUN ONCE!
# convert scientific (Latin) name to gbif taxon ID
# (Good to avoid synonyms, which are common with english species names)
speciesKey <- rgbif::name_backbone('Loxia scotica')$speciesKey

# get the occurrence records of this species
gbif_response <- occ_search(
  scientificName = "Loxia scotica",
  country = "GB",
  hasCoordinate = TRUE,
  hasGeospatialIssue = FALSE,
  limit = 9999)
# backup to reduce API load
write_rds(
  x = gbif_response,
  path = here::here('data/gbif_occs_loxsco.rds')
)
# # to read file back in:
# gbif_response <- read_rds(path = here::here('data/gbif_response_loxsco.rds'))
```


GBIF and rOpenSci just saved us years or roaming around the highlands with a 
pair of binoculars, camping in mud, rain, and snow, and chasing crossbills 
through the forest. Nevertheless, it is still up to us to make sense of the
data we got back, in particular to clean it, as data collected on this large
scale can have its own issues. Luckily, GBIF provides some useful metadata
on each record. Here, I will exclude those that

* are not tagged as "present" (they may be artifacts from collections)
* don't have any flagged issues (nobody has noticed anything abnormal with this)
* are under creative commons license (we can use them here)
* are older than 1965


```{r data cleaning}
# look at a random sub-sample of the data (100 rows)
# we can see there is a lot of information on each record!
gbif_response$data %>% sample_n(100)

birds_clean <- gbif_response$data %>%
  # get decade of record from eventDate
  mutate(decade = eventDate %>% 
           ymd_hms() %>% 
           round_date("10y") %>%
           year() %>%
           as.numeric()) %>%
  # clean data using metadata filters
  filter(
    # only records with no issues
    issues == "" &
    # only creative commons license records
    str_detect(license, "http://creativecommons.org/") &
    # no records before 1965
    decade >= 1970 &
    # no records after 2015 (there is not a lot of data yet)
    decade < 2020) %>%
  # retain only relevant variables
  # (note: select is a name conflict between the raster package the dplyr.
  # I am therefore explicitly calling dplyr::select here)
  dplyr::select(decimalLongitude, decimalLatitude, decade) %>%
  # sort by decade
  arrange(decade)

```


Okay so we've got clean data now. Here comes the nifty trick. We want to look
at the data in subsets by decade in order to see if and how the spatial 
distribution of the species has changed over time. To do so, we can "nest"
data in a list column using the `tidyr` package:

```{r}

birds_nested <- birds_clean %>%
  # define the nesting index
  group_by(decade) %>% 
  # aggregate data in each group
  nest()

# let's have a look
head(birds_nested)

```

The species data needs some further transformation before we can use it in this 
project. Let's extract the decade of the records from the date field using the
`lubridate` package. Now that the data is cleaned we can also exclude a lot of 
the metadata shift our focus to the spatial information. This is given as 
latitude and longitude but since we're only looking at the UK (and because
our environmental data is from the UK Met Office), we should transform this to
a UK grid projection. This is easily done using the `sp` package. If you want 
to learn more about map projections you can check out 

TODO: enter xkcd reference and a good link

```{r}

# latlon crs reference
proj_latlon <- CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0")
proj_ukgrid <- CRS("+init=epsg:27700")

birds_nested <- birds_nested %>%
  # make records into spatial points
  mutate(points = map(
    .x = data, 
    .f = ~ SpatialPoints(coords = .x, proj4string = proj_latlon) %>%
      # reproject spatial points to the UK grid
      # (this is so it matches our climate data)
      spTransform(CRSobj = proj_ukgrid)))

```


We also need climate data for our analysis. I have prepared this separately
based on a dataset provided by the UK Met Office. It is nested in the same way
as the bird data (although I admit that nesting weather seems slighly less
straight forward than nesting birds).

If you would like to learn more about raster data and how to work with it you
can check out the [datacamp course on spatial analysis in R](https://www.datacamp.com/courses/spatial-analysis-in-r-with-sf-and-raster)

```{r}

# read raster data
climate_nested <- read_rds(here::here("data/ukcp09_stacked_rasters.rds"))
# plot 1970
plot(climate_nested$raster_stacks[[1]])

```

```{r}

# ggplot raster
clim <- climate_nested$raster_stacks[[4]] %>%
  as("SpatialPixelsDataFrame") %>%
  as_data_frame() %>%
  na.omit() %>%
  gather(key = "variable", value = "value", -x, -y) %>%
  group_by(variable) %>%
  nest()

plot_clim <- function(df, var) {
  df %>%
    ggplot(aes(x = x, y = y, fill = value)) +
    geom_tile() +
    scale_fill_distiller(palette = "Spectral") + 
    coord_equal() +
    theme_map() +
    theme(legend.position = "bottom") +
    labs(title = var, caption = 'Source: MetOffice UK climate data')
  }

clim2 <- mutate(clim, plots = map2(.x = data, .y = variable, .f = plot_clim))
clim2$plots[[1]]
x <- gridExtra::grid.arrange(grobs = clim2$plots)

ggsave(
  plot = x, 
  filename = "02_climate_plot.png", 
  device = "png",
  width = 12, 
  height = 18, 
  scale = 1, 
  dpi = 150)

```

For our purpose of model fitting we will combine the corresponding bird 
and climate datasets by joining them together using decade. This allows us to
subsequently extract the climatic conditions at a location that the bird was
observed at, in the decade that it was observed.

```{r}

# check that the two datasets match up
stopifnot(all(climate_nested$decade == birds_nested$decade))

# combine climate and bird data
locs_withbirds <- data_frame(
  decade = birds_nested$decade,
  presence = 1,
  climate =  map2(
    .x = climate_nested$raster_stacks,
    .y = birds_nested$points,
    .f = ~ as_data_frame(raster::extract(.x, .y))))

# get number of records per decade
n_points <- map_dbl(.x = birds_nested$data, .f = nrow)
# create a random sample from the rasters to use as absence data
locs_nobirds <- data_frame(
  decade = birds_nested$decade,
  presence = 0,
  # draw random sample from climate data
  climate = map2(
    .x = climate_nested$raster_stacks,
    .y = n_points * 5,
    .f = ~ as_tibble(sampleRandom(.x, size = .y, cells = FALSE))))

# combine the two datasets
df_nested <- bind_rows(locs_withbirds, locs_nobirds) %>% 
  unnest() %>% 
  group_by(decade) %>% 
  nest()

df_nested

```



```{r train_test_split}

# last pre-processing step
df_modelling <- df_nested %>%
  # get into modelling format
  unnest() %>%
  # caret requires a factorial response variable for classification
  mutate(presence = case_when(
    presence == 1 ~ "presence",
    presence == 0 ~ "absence") %>%
    factor()) %>%
  # drop all observations with NA variables
  na.omit()

# create a training set for the model build
df_train <- df_modelling %>%
  # true temporal split as holdout
  filter(decade != "2010") %>%
  # drop decade, it's not needed anymore
  dplyr::select(-decade)

# same steps for test set
df_test <- df_modelling %>%
  filter(decade == "2010") %>%
  dplyr::select(-decade)

```


Caret makes the model fitting incredibly easy! All we need to do is specify
a tuning grid of hyperparameters that we want to optimise, a tune control 
that adjusts the number of iterations and the loss function used, and then
call train with the algorithm we have picked.

```{r model_fitting}
# for reproducibility
set.seed(12345)

# set up model fitting parameters
# tuning grid, trying every possible combination
tuneGrid <- expand.grid(
  alpha = seq(0, 1, length = 5),
  lambda = c(.001, .003, .01, .03, .1, .3))
tuneControl <- trainControl(
  method = 'repeatedcv',
  classProbs = TRUE,
  number = 10,
  repeats = 1,
  verboseIter = TRUE,
  summaryFunction = twoClassSummary)
# actual model build
model_fit <- train(
  presence ~ .,
  data = df_train,
  method = "glmnet",
  family = "binomial",
  metric = "ROC",
  tuneGrid = tuneGrid,
  trControl = tuneControl)


plot(model_fit)

```

```{r model_fitting}
# for reproducibility
set.seed(12345)
library(RRF)
# set up model fitting parameters
# tuning grid, trying every possible combination
tuneGrid <- expand.grid(
  mtry = c(3, 6, 9),
  coefReg = c(.01, .03, .1, .3, .7, 1),
  coefImp = c(.0, .1, .3, .6, 1))
tuneControl <- trainControl(
  method = 'repeatedcv',
  classProbs = TRUE,
  number = 10,
  repeats = 2,
  verboseIter = TRUE,
  summaryFunction = twoClassSummary)
# actual model build
model_fit <- train(
  presence ~ .,
  data = df_train,
  method = "RRF",
  metric = "ROC",
  tuneGrid = tuneGrid,
  trControl = tuneControl)


plot(model_fit)

```

We can evaluate the performance of this model on our hold-out data from 2010.
Just as uring training we are using the Area under the Receiver Operator 
Characteristic curve (AUC). With this metric, a model no bettern than random
would score 0.5 while a perfect model making no mistakes would score 1.

```{r model_evaluation}

# combine prediction with validation set
df_eval <- data_frame(
  "obs" = df_test$presence,
  "pred" = predict(
    object = model_fit,
    newdata = df_test,
    type = "prob") %>%
    pull(1))

# get ROC value
roc_auc_vec(estimator = "binary", truth = df_eval$obs, estimate = df_eval$pred)

```


That is a pretty good result! 

```{r}

model_fit <- update(
  object = model_fit,
  param = list(mtry = 6, coefReg = 0, coefImp = .6))

```


Now we can combine the raw data, model performance, and predictions all in one
nested dataframe. We can save this for later to make sure we always know what
data was used to build which model.

```{r model_prediction}

df_eval <- df_modelling %>% 
  group_by(decade) %>% nest() %>%
  # combine with climate data
  left_join(climate_nested, by = "decade") %>%
  # evaluate by decade
  mutate(
    "obs" = map(
      .x = data, 
      ~ .x$presence),
    "pred" = map(
      .x = data, 
      ~ predict(model_fit, newdata = .x, type = "prob") %>% pull("presence")),
    "auc" = map2_dbl(
      .x = obs,
      .y = pred,
      ~ roc_auc_vec(
          estimator = "binary",
          truth = .x, 
          estimate = .y)),
    "climate_data" = map(
      .x = raster_stacks, 
      ~ as(.x, "SpatialPixelsDataFrame") %>%
        as_data_frame() %>%
        na.omit()),
    "habitat_suitability" = map(
      .x = climate_data,
      ~ predict(model_fit, newdata = .x, type = "prob") %>% pull("presence"))
    )

df_eval

```


```{r create_animation}

# remove cells with missing values in any decade
# (otherwise mismatch causes animation glitches)
no_na_data <- df_eval %>%
  dplyr::select(decade, climate_data, habitat_suitability) %>% 
  # change nesting
  unnest() %>% group_by(x, y) %>% nest() %>%
  # find cells with missing values
  mutate(any_na = map_lgl(
      .x = data,
      ~ dim(.x)[[1]] == 5)) %>%
  # exclude them
  filter(any_na) %>%
  dplyr::select(x, y, data) %>%
  unnest()

# create animation cycling through decades
hs_animation <- no_na_data %>%
  dplyr::select(x, y, decade, habitat_suitability) %>%
  gather(key = "variable", value = "value", -x, -y, -decade) %>%
  filter(variable == "habitat_suitability") %>%
  # plot data
  ggplot(aes(x = x, y = y, fill = value)) +
  geom_tile() +
  # good colour palette for colour blind people
  scale_fill_viridis(option = "A") +
  coord_equal() +
  theme_map() +
  theme(legend.position = "bottom") +
  # animate plot, morphing through decades
  transition_states(
    states = decade,
    transition_length = 5,
    state_length = 2) +
  labs(title = 'Habitat Suitability per Decade',
    # gganimate: animate subtitle
    subtitle = 'Decade: {closest_state}',
    caption = 'Source:\nGBIF data and\nMetOffice UK climate data',
    fill = 'Habitat Suitability [0 low - high 1]')

hs_animation
```